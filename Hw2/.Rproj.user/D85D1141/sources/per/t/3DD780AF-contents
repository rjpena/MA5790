---
title: "Chapter 3 Exercises"
author: "Ruben Pena"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
  pdf_document: default
  word_document: default
  html_notebook: default
---
  MA5790 - Dr. Qiuying Sha    
  Exercises 3.1, 3.2, 3.3(-b)   
  Due September 28, 2018  
  Data Pre-processing, R code, and Analysis.
```{r}
#install packages
install.packages(c("crayon","knitr","VIM","mlbench","caret","magrittr"))
#attach these packages and suppress message
(library(mlbench))
(library(knitr))
(library(crayon))
```

```{r}
#call the data from the mlbench library
data(Glass)
#display dimension and column names of Glass 
dim(Glass)
names(Glass)
```



## Part 3.1 Glass Dataset
### A) *Using visualizations, explore the predictor variables to understand their distrubitons as well as the relationships betewen predictors.*

##### Relationships Between Variables

The code chunk below subsets the columns containing numeric values and places them, by column(elements that make glass), into a scatter plot matrix in order to get a rough estimation of linearity between predictor sets.
```{r fig1, fig.height = 6, fig.width = 8}
#subset numeric values in all rows, columns 1-9
num<-Glass[,1:9]
#create a table of scatter plots using subset of Glass data containing numerics only
plot(num,main="Scatterplot Matrix of Numeric Predictors") 
```
#### Analyis of Scatterplot Matrix
Based on the figure output by the plot function above, it appears that there is a strong positive correlation between the variable pair of Rl and Ca. The pair set Rl and Si seem to have a weak or moderate negative correlation based on their respective graphs. The rest of the variable sets appear to have no correlation between them.

##### Data Distributions
The code chunk below creates a figure of histograms with a 3x3 axes (for all 9 columns in one figure) to get a quick visualization of distribution of data points among the variables.
```{r fig2, fig.height = 6, fig.width = 8}
#set parameter for graphics to 3x3 figure
par(mfrow = c(3,3)) 
#iterate through columns in num creating histograms of data and naming each as the column name + Histogram
for (x in 1:ncol(num)) {
  hist(num[ ,x],xlab="", main = paste(names(num[x]), "Histogram"))
} 
```
#### Analysis of Histograms
"Rl" - Fairly symmetrical with a peak near 1.515. The data for this predictor appears to be normally distributed.  
"Na" - Fairly symmetrical and peaks around 14 with a gap at 17.  The data for this predictor appears to be normally distributed.  
"Mg" - Asymmetrical with peaks at 0 and 4.  The data for this predictor appears to not be normally distributed.  
"Al" - Fairly symmetrical with a peak at 1.0. The data for this predictor appears to be normally distributed.   
"Si" - Fairly symmetrical with a peak at 73. The data for this predictor appears to be normally distributed.  
"K"  - Asymmetric with a peak at 1 and possible extreme values at 6. The data for this predictor appears to not be normally distributed.    
"Ca" - Fairly symmetrical with a peak at 8. The data for this predictor appears to be normally distributed.   
"Ba" - Asymmetric with a peak at 0. The data for this predictor appears to not be normally distributed.   
"Fe" - Asymmetric with a peak at 0. The data for this predictor appears to not be normally distributed.    

### B) *Do there appear to be any outliers in the data? Are any predictors skewed?*
The code chunk below will create boxplots of the predictor data in order to assess both skewness of the data and outliers.
```{r fig3, fig.height = 6, fig.width = 8}
#set parameter for graphics to 3x3 figure
par(mfrow = c(3,3)) 
#iterate through columns in num creating box-and-whisker plots of the data and naming each as the column name + Boxplot
for (x in 1:ncol(num)) {
  boxplot(num[ ,x], horizontal=TRUE, main = paste(names(num[x]), "Boxplot"))
}
```
#### Analysis of Boxplots
##### Outliers
Boxplots are a great resource for visualizing the quartiles of the data(descriptive statistics). The box contains data points within the 25th percentile and the 75th percentile, also known as the 1st and 3rd quartiles. The "whiskers" that extend from the box, also known as "fences" show the cutoff point for outliers. Data points that lie beyond the fences are considered to be outliers.

All elements in the subeset of data appear to have outlier values with the exception of "Mg". Elements "K", "Ba", and "Fe" have a min of zero and many data points near that value. These elements contain no oultiers beyond the lower fence, only beyond the upper fence."Rl", "Na", "Al", "Si", and "Ca" have outliers beyond both upper and lower fence.

##### Skewness
The following predictors appear to be normally distributed without significant skewing: "Rl","Na","Al", and "Si". 

The "Mg" predictor looks skewed to the left with a separate peak near the 0.

Predictors "K", "Ba", "Ca", and "Fe" are all skewed to the right.

This is evident in both the boxplots and the histograms. 

### C) *Are there any relevant transformations of one or more predictors that might improve the classification model?*

As there are many skewed predictors with assymetrical data distributions, a Box-Cox power transformation would improve the model by normalizing the data based on the best transformation it finds; log or square root transformations, if needed.


## Part 3.2 Soybean Dataset

```{r}
#attach corrplot lib
suppressPackageStartupMessages(library(corrplot))
#import soybean data
data(Soybean)
#display dimensions and column names of Soybean
str(Soybean)
```

### A) *Investigate the frequency distribution for the categorical predictors. Are any of the distributions degenerate in the ways discussed earlier in this chapter?*

Degenerate distributions, as covered in class, are commonly known as zero or near-zero variance predictors because they have very few unique values and/or frequencies. It can be advantageous to filter these predictors from the dataset in order to increase model performance as these predictors rarely describe the target response. In order to check for zero variance or near-zero variance, a method in R can be used to suss these out based upon the categorical data in the Soybean dataset. First, the target variable must be removed.
```{r}
# remove the first column of Soybean
data <- Soybean[,-1]
#print structure of data
str(data)
```

Now that the target variable has been removed, the nearzero function can be used to identify predictors with with low variance.

```{r}
#run nearZeroVar on data. Output is columns with near zero variance.
library(caret)
NZV<-c(nearZeroVar(data,names=TRUE))
NZV
```
This code shows the number of 0's in "leaf.mild". Having 535 of the same values based on 683 observations confirms near zero variance in that predictor; making it less useful in modeling.
```{r}
#how many in the subset of data of leaf.mild are equal to 0, not 1 or 2.
length(which(data$leaf.mild==0))
```
Based on the evaluation of the nearZeroVar function, columns 18, 25, and 27 are degenerate. A quick check shows that 535 of 683 variables in column 18 are zero.

It should be noted that with categorical data there can be a tendency to have little variance in the predictors. In this case the categorical data has very few factors; in most cases there are only 2 or 3 options. This will drastically reduce variance for these predictors as the choices are limited.


### B) *Roughly 18% of the data are missing. Are there particular predictors that are more likely to be missing? Is the pattern of missing data related to the classes?*

The *summary* function of R is a quick and easy way to get a sum of all NA's in a dataframe. The output below shows all of the value counts for each predictor as well as the sum of NA's. The *colSums()* paired with the *is.na()* is also a quick way to sum the total of NA's in each column.

```{r}
#print summary of Soybean df
summary(Soybean)
#sum column value counts of na and print results
mv<-colSums(is.na(Soybean))
mv
```
The *VIM* package contas an aggregating function that plots missing or imputed values for each variable and for combinations. This provides a graphical way to analyze missing values by variable combinations. Below are two graphics showing the proportion of missing values per variable as well as the combinations of missing values per variable. As stated in the text book the dataset contains roughly 18% missing values (17.71% according to *VIM*) and all records have a Class designation(output variable). The *aggr* function is set to sort the variables from highest propotion of missing values (hail) to the lowest(Class). 
```{r}
library(VIM)
#aggr is from the VIM package. It plots and calculates amount of mv's for each variable and for combinations of variables. (?aggr)
aggr(Soybean, bars=T, prop=T, combined=F, numbers=T, sortVars=T)
```
The *aggr* is a simple way to view proportion of missing values, but in order to check for a relation of missing values to Class, the *dplyr* package from *tidyverse* offers grammatic manipulation of dataframes using mutation, selection, filtering, and grouping (similar to a structured query language). This package is not inherently easy to understand. 

Mutate creates new variables derived from existing variables that have had some function applied to them. In this instance a "Total" variable was created using the total number of observations in Soybeans. 

Filter was used to filter observations *with* missing values (those observations that are not complete for all columns)

Group_by is set to Class as that is the proportion of interest.

Mutate was used again to take the Number of missing values for each class and sore it in the Number_Missing variable. Mutate created a Proportion variable using the Number_Missing compared to the total observations. 

Similar to SQL Select designates which column to show(grouped by class) with duplicates removed (unique). 

This process shows that the Class *phytophthora-rot* contans a significant proportion of missing values among class. This could be a result of structurally missing data. For example this particular plant may not have seeds to measure the size or fruit spots to be measured.
```{r}
#attach dplyer, from VIM, a datafram manipulator based on plyr
library(dplyr)
library(magrittr)
Soybean %>%
  mutate(Tot = n()) %>% 
  filter(!complete.cases(.)) %>%
  group_by(Class) %>%
  mutate(Number_Missing = n(), Proportion=Number_Missing/Tot) %>%
  select(Class, Number_Missing, Proportion) %>%
  unique()
```

### C) *Develop a strategy for handling missing data, either by eliminating predictors or imputation.*

For this particular dataset I would start by removing the near zero variances identified earlier. 

```{r}
Soybean$leaf.mild <- NULL
Soybean$mycelium <- NULL
Soybean$sclerotia <- NULL
str(Soybean)
```
Now that we have removed the predictors with near zero variance, a kNN model can be used for imputation. In this instance, the 3 nearest neighbors were chosen to avoid overfitting to imputations as this is a somewhat small dataset. Below is the summary of the new dataset showing no NA's.


```{r}
#store kNN imputation in Soy_1 variable. Impute for NAs in all cols using 3 nearest neighbors, do not append logical improvement variables
Soy_1<-kNN(Soybean,variable=colnames(Soybean),k=3,imp_var=F)
```
```{r}
summary(Soy_1)
colSums(is.na(Soy_1))
```

## Part 3.3 Glass Dataset
### A) *Start R and use these commands to load the data:*
```{r}
data(BloodBrain)
?BloodBrain
```
### *The numeric outcome is contained in the vector logBBB*
```{r}
str(logBBB)
```
### *While the predictors are in the data frame bbbDescr.*
```{r}
str(bbbDescr)
```

###B)*Generally speaking, are there strong relationships between the predictor data?* 
```{r}
#get correlation coefficients for bbbDescr
corrs=cor(bbbDescr)
#plot corrs using hierarchichal clustering
corrplot(corrs,order="hclust")
```
Based on the ouptut of the correlation plot there appears to be significant correlations(both positive and negative) in the predictor data.



###*If so, how could correlations in the predictor set be reduced?* 
Correlations in the predictor set could be reduced using by finding correlation values that meet or exceed a defined threshold using the *findCorrelation()*. For example:

```{r}
#find correlations with values >= 85% and store in highCorrs variable
highCorrs <- findCorrelation(corrs,cutoff=.85)
highCorrs 
```
The correlations that meet or exceed the cutoff value could then be removed from the data.

```{r}
#print length of bbbDescr and highCorrs 
#to test if they are removed later
length(highCorrs)
length(bbbDescr)
```

```{r}
#remove highly correlated predictors (at or above 85% cutoff value)
bbbDesc_1 <- bbbDescr[,-highCorrs]
```
###*Does this have a dramatic effect on the number of predictors available for modeling?*
Yes, this reduces the number of predictors by 49 from 134 to 85.

```{r}
#length of bbbDescr data frame after hightly correlated predictors are removed.
length(bbbDesc_1)
```


###Sources

DPLYR - https://dplyr.tidyverse.org/
aggr - https://www.rdocumentation.org/packages/AggregateR/versions/0.0.2
NearZeroVar - Class notes provided by Dr. Sha
kNN - https://www.rdocumentation.org/packages/VIM/versions/4.7.0/topics/kNN




